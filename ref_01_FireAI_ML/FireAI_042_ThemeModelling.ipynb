{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(37) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programsoft\\python\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# 准备数据集，建一个class来加载数据集，对数据进行预处理\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "\n",
    "class DataSet:\n",
    "    \n",
    "    def __init__(self,txt_file_path):\n",
    "        self.__txt_file=txt_file_path\n",
    "    \n",
    "    def __load_txt(self): # 从txt文档中加载文本内容，逐行读入\n",
    "        with open(self.__txt_file,'r') as file:\n",
    "            content=file.readlines() # 一次性将所有的行都读入\n",
    "        return [line[:-1] for line in content] # 去掉每一行末尾的\\n\n",
    "    \n",
    "    def __tokenize(self,lines_list): # 预处理之一：对每一行文本进行分词\n",
    "        tokenizer=RegexpTokenizer('\\w+') \n",
    "        # 此处用正则表达式分词器而不用word_tokenize的原因是：排除带有标点的单词\n",
    "        return [tokenizer.tokenize(line.lower()) for line in lines_list]\n",
    "    \n",
    "    def __remove_stops(self,lines_list): # 预处理之二：对每一行取出停用词\n",
    "        # 我们要删除一些停用词，避免这些词的噪声干扰，故而需要一个停用词表\n",
    "        stop_words_list=stopwords.words('english')  # 获取英文停用词表\n",
    "        return [[token for token in line if token not in stop_words_list]\n",
    "                for line in lines_list] \n",
    "        # 这儿有点难以理解，lines_list含有的元素也是list，这一个list就是一行文本，\n",
    "        # 而一行文本内部有N个分词组成，故而lines_list可以看出二维数组，需要用两层generator\n",
    "    \n",
    "    def __word_stemm(self,lines_list): # 预处理之三：对每个分词进行词干提取\n",
    "        stemmer=SnowballStemmer('english')\n",
    "        return [[stemmer.stem(word) for word in line] for line in lines_list]\n",
    "    \n",
    "    def prepare(self):\n",
    "        '''供外部调用的函数，用于准备数据集'''\n",
    "        # 先从txt文件中加载文本内容，再进行分词，再去除停用词，再进行词干提取\n",
    "        stemmed_words=self.__word_stemm(self.__remove_stops(self.__tokenize(self.__load_txt())))\n",
    "        # 后面的建模需要用到基于dict的词矩阵，故而先用corpora构建dict在建立词矩阵\n",
    "        dict_words=corpora.Dictionary(stemmed_words)\n",
    "        matrix_words=[dict_words.doc2bow(text) for text in stemmed_words]\n",
    "        return dict_words, matrix_words \n",
    "    \n",
    "    # 以下函数主要用于测试上面的几个函数是否运行正常\n",
    "    def get_content(self):\n",
    "        return self.__load_txt()\n",
    "    \n",
    "    def get_tokenize(self):\n",
    "        return self.__tokenize(self.__load_txt())\n",
    "    \n",
    "    def get_remove_stops(self):\n",
    "        return self.__remove_stops(self.__tokenize(self.__load_txt()))\n",
    "    \n",
    "    def get_word_stemm(self):\n",
    "        return self.__word_stemm(self.__remove_stops(self.__tokenize(self.__load_txt())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['He spent a lot of time studying cryptography. ', 'You need to have a very good understanding of modern encryption systems in order to work there.', \"If their team doesn't win this match, they will be out of the competition.\"]\n",
      "[['he', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'cryptography'], ['you', 'need', 'to', 'have', 'a', 'very', 'good', 'understanding', 'of', 'modern', 'encryption', 'systems', 'in', 'order', 'to', 'work', 'there'], ['if', 'their', 'team', 'doesn', 't', 'win', 'this', 'match', 'they', 'will', 'be', 'out', 'of', 'the', 'competition'], ['those', 'codes', 'are', 'generated', 'by', 'a', 'specialized', 'machine'], ['the', 'club', 'needs', 'to', 'develop', 'a', 'policy', 'of', 'training', 'and', 'promoting', 'younger', 'talent'], ['his', 'movement', 'off', 'the', 'ball', 'is', 'really', 'great'], ['in', 'order', 'to', 'evade', 'the', 'defenders', 'he', 'needs', 'to', 'move', 'swiftly'], ['we', 'need', 'to', 'make', 'sure', 'only', 'the', 'authorized', 'parties', 'can', 'read', 'the', 'message']]\n",
      "[['spent', 'lot', 'time', 'studying', 'cryptography'], ['need', 'good', 'understanding', 'modern', 'encryption', 'systems', 'order', 'work'], ['team', 'win', 'match', 'competition'], ['codes', 'generated', 'specialized', 'machine'], ['club', 'needs', 'develop', 'policy', 'training', 'promoting', 'younger', 'talent'], ['movement', 'ball', 'really', 'great'], ['order', 'evade', 'defenders', 'needs', 'move', 'swiftly'], ['need', 'make', 'sure', 'authorized', 'parties', 'read', 'message']]\n",
      "[['spent', 'lot', 'time', 'studi', 'cryptographi'], ['need', 'good', 'understand', 'modern', 'encrypt', 'system', 'order', 'work'], ['team', 'win', 'match', 'competit'], ['code', 'generat', 'special', 'machin'], ['club', 'need', 'develop', 'polici', 'train', 'promot', 'younger', 'talent'], ['movement', 'ball', 'realli', 'great'], ['order', 'evad', 'defend', 'need', 'move', 'swift'], ['need', 'make', 'sure', 'author', 'parti', 'read', 'messag']]\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(13, 1), (14, 1), (15, 1), (16, 1)], [(17, 1), (18, 1), (19, 1), (20, 1)], [(8, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)], [(28, 1), (29, 1), (30, 1), (31, 1)], [(8, 1), (9, 1), (32, 1), (33, 1), (34, 1), (35, 1)], [(8, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# 检验上述DataSet类是否运行正常\n",
    "dataset=DataSet(\"E:\\PyProjects\\DataSet\\FireAI\\data_topic_modeling.txt\")\n",
    "\n",
    "# 以下测试load_txt()函数是否正常\n",
    "content=dataset.get_content()\n",
    "print(len(content))\n",
    "print(content[:3])\n",
    "\n",
    "# 以下测试__tokenize()函数是否正常\n",
    "tokenized=dataset.get_tokenize()\n",
    "print(tokenized)\n",
    "\n",
    "# 一下测试__remove_stops()函数是否正常\n",
    "removed=dataset.get_remove_stops()\n",
    "print(removed)\n",
    "\n",
    "# 以下测试__word_stemm()函数是否正常\n",
    "stemmed=dataset.get_word_stemm()\n",
    "print(stemmed)\n",
    "\n",
    "# 以下测试prepare函数是否正常\n",
    "_,prepared=dataset.prepare()\n",
    "print(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上测试表名DataSet类运行正常，\n",
    "# 所准备的词矩阵也是符合预期，可以用于后续建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据集\n",
    "dataset=DataSet(\"E:\\PyProjects\\DataSet\\FireAI\\data_topic_modeling.txt\")\n",
    "dict_words, matrix_words =dataset.prepare()\n",
    "\n",
    "# 使用LDAModel建模\n",
    "lda_model=models.ldamodel.LdaModel(matrix_words,num_topics=2,\n",
    "                           id2word=dict_words,passes=25) \n",
    "# 此处假设原始文档有两个主题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important words to topics: \n",
      "Topic: 0, words: 0.075*\"need\" + 0.053*\"order\" + 0.032*\"system\" + 0.032*\"encrypt\" + 0.032*\"work\"\n",
      "Topic: 1, words: 0.037*\"younger\" + 0.037*\"develop\" + 0.037*\"promot\" + 0.037*\"talent\" + 0.037*\"train\"\n"
     ]
    }
   ],
   "source": [
    "# 查看模型中最重要的N个单词\n",
    "print('Most important words to topics: ')\n",
    "for item in lda_model.print_topics(num_topics=2,num_words=5):\n",
    "    # 此处只打印最重要的5个单词\n",
    "    print('Topic: {}, words: {}'.format(item[0],item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
