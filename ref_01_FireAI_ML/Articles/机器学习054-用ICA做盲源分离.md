【火炉炼AI】机器学习054-用ICA做盲源分离
-

(本文所使用的Python库和版本号: Python 3.6, Numpy 1.14, scikit-learn 0.19, matplotlib 2.2 )

盲源分离是指在信号的理论模型和源信号无法精确获知的情况下，如何从混叠信号中分离出各源信号的过程。盲源分离的目的是求得源信号的最佳估计。说的通俗一点，就相当于，假如有十个人同时说话，我用录音机把他们说的话都录下来，得到的肯定是10种声音的混杂，那么怎么将这种混杂声音分离成单个人的说话声音？解决类似这种问题就是盲源分离。

独立成分分析（Independent Components Analysis, ICA)解决的是原始数据分解的问题，常常用于盲源分离问题中。在我上一篇文章[【火炉炼AI】机器学习053-数据降维绝招-PCA和核PCA](https://www.cnblogs.com/RayDean/p/9882023.html)中提到PCA虽然具有各种优点，但是也有几个缺点，比如不能对非线性组织的数据集降维，针对这个缺点解决方法是用核PCA代替PCA，另外一个缺点是不能用于解决数据集不满足高斯分布的情况，这种情况的数据降维要用独立成分分析ICA来完成。

独立成分分析师从多维统计数据中寻找潜在因子或成分的一种方法，ICA与PCA等降维方法的区别在于，它寻找满足统计独立和非高斯的成分。其数学原理和逻辑可以参考博文：[独立成分分析ICA系列2：概念、应用和估计原理](https://blog.csdn.net/shenziheng1/article/details/53637907)


<br/>

## 1. 加载数据集

首先加载数据集，本次所用的数据集位于文件mixture_of_signals.txt中，这个文件中有四列数据，代表四个不同的信号源，共2000个样本

```py
data_path="E:\PyProjects\DataSet\FireAI\mixture_of_signals.txt"
df=pd.read_csv(data_path,header=None,sep=' ')
print(df.info()) # 查看数据信息，确保没有错误
print(df.head())
print(df.tail())
dataset_X=df.values
print(dataset_X.shape)
```

绘图后，可以看出这些数据的分布情况：

![](https://i.imgur.com/hQ7k80i.png)


<br/>

## 2. 用传统PCA来分离信号

假如我们用PCA来进行盲源分离，可以看看效果怎么样，代码为：

```py
# 如果用PCA来进行分离，看看结果如何
from sklearn.decomposition import PCA
pca = PCA(n_components=4)
pca_dataset_X = pca.fit_transform(dataset_X) 
pd.DataFrame(pca_dataset_X).plot(title='PCA_dataset')
```

![](https://i.imgur.com/VLfbUnP.png)

上面虽然绘制了PCA分离之后的各种信号，但是信号夹杂在一起难以分辨，故而我编写了一个函数将其分开显示

```py
def plot_dataset_X(dataset_X):
    rows,cols=dataset_X.shape
    plt.figure(figsize=(15,20))
    for i in range(cols):
        plt.subplot(cols,1,i+1)
        plt.title('Signal_'+str(i))
        plt.plot(dataset_X[:,i])
        
```

![](https://i.imgur.com/2ATRIdN.png)


<br/>

## 3. 用ICA来分离信号

下面看看用独立成分分析方法得到的分离后信号：

```py
# 如果用ICA进行信号分离
from sklearn.decomposition import FastICA
ica = FastICA(n_components=4)
ica_dataset_X = ica.fit_transform(dataset_X)
pd.DataFrame(ica_dataset_X).plot(title='ICA_dataset')
```

![](https://i.imgur.com/wj7Pmll.png)

同理，为了显示方便，将各种信号单独画图，如下：

![](https://i.imgur.com/CHbBhHQ.png)

可以看出，经过ICA分离之后得到的信号非常有规律，而PCA分离后的信号有些杂乱，表面ICA的盲源分离效果较好。

**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#小\*\*\*\*\*\*\*\*\*\*结\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**

**1，用ICA可以解决盲源分离问题，所得到的分离效果要比PCA要好得多。**

**2，实际上，生活中的真实数据集大部分都不是服从高斯分布，它们一般服从超高斯分布或亚高斯分布，故而很多问题用PCA得到的效果不太理想，返回用ICA能够得到比较好的结果。**

**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**

<br/>


注：本部分代码已经全部上传到（[**我的github**](https://github.com/RayDean/MachineLearning)）上，欢迎下载。

参考资料:

1, Python机器学习经典实例，Prateek Joshi著，陶俊杰，陈小莉译