【火炉炼AI】机器学习034-NLP对文本进行分词
-

(本文所使用的Python库和版本号: Python 3.6, Numpy 1.14, scikit-learn 0.19, matplotlib 2.2， NLTK 3.3， jieba 0.39)

分词过程可以认为是自然语言处理（NLP）的第一步，在我们获取了文本数据集后，首先要做的就是将文本句子分割成各种单词，下面介绍各种常用的分词工具。

<br/>

## 1. 对英文进行分词

### 1.1 对句子进行分割--sent_tokenize

NLP可以对句子进行分割，也就将一整段文本分割成几句话，经常是以句子结尾符号为标志来分割。这些符号包括有问号，感叹号，句号等。（逗号不会分割。）如下演示代码


```Python
# 对句子进行分割（tokenization)
from nltk.tokenize import sent_tokenize
text = "Are you curious about tokenization? Let's see how it works! We need to analyze, a couple of sentences with punctuations to see it in action."
sent_list=sent_tokenize(text)
print(sent_list) # 句子结尾符号为标志分割
```

**-------------------------------------输---------出--------------------------------**

['Are you curious about tokenization?', "Let's see how it works!", 'We need to analyze, a couple of sentences with punctuations to see it in action.']

**--------------------------------------------完-------------------------------------**

可以看出sent_tokenize以问号，感叹号，句号来分割，但是中间的逗号却留在原来的句子中，没有分割。

### 1.2 对句子进行分词--word_tokenize

对句子进行分词，是将一段文本，一句话或者几个单词分割成几个单独的单词，分割的依据是空格，问号，逗号，句号等符号，代码如下：

```Python
# 对句子进行分词，即根据语义将一句话分成多个单词
from nltk.tokenize import word_tokenize
text = "Are you curious about tokenization? Let's see how it works! We need to analyze, a couple of sentences with punctuations to see it in action."
word_list=word_tokenize(text)
print(word_list) # 以空格，问号，感叹号，句号等分割成单词
# 分词在NLP中非常重要，经常是NLP的第一步
```

**-------------------------------------输---------出--------------------------------**

['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', "'s", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', ',', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']

**--------------------------------------------完-------------------------------------**


### 1.3 对句子进行分词--WordPunctTokenizer

WordPunctTokenizer的结果基本和上面的word_tokenize结果一致，不同的地方在于WordPunctTokenizer分词器可以将标点符号保留到不同的句子标记中。代码如下：

```Python
# 还有一个分词方法：WordPunct分词器，可以将标点符号保留到不同的句子标记中
from nltk.tokenize import WordPunctTokenizer
text = "Are you curious about tokenization? Let's see how it works! We need to analyze, a couple of sentences with punctuations to see it in action."
word_punct=WordPunctTokenizer()
word_punct_list=word_punct.tokenize(text)
print(word_punct_list)
```

**-------------------------------------输---------出--------------------------------**

['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', "'", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', ',', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']

**--------------------------------------------完-------------------------------------**

通过和上面的word_tokenize结果结果进行比较可以发现，唯一的不同就是Let's分割时不同，WordPunctTokenizer将Let's分割为三个单词，而word_tokenize却分割成两个单词。一般情况下，这一点区别意义不大，故而word_tokenize用得跟多一些。

**（注意：虽然在《Python机器学习经典实例》中提到还有一个分词器PunktWordTokenizer，但是这个分词器已经弃用了，就连import都会出错，故而此处不讲解。）**

## 2. 对中文进行分词

上面的几个分词器对英文非常好用，但是很遗憾，对中文不能分词成功，我自己尝试过。对英文的分词很简单，因为是以空格作为自然分解符，可是中文却不是的，中文要分成单词而不是汉字。

对于中文分词，有很多种分词模块，比如：jieba、SnowNLP（MIT）、pynlpir（大数据搜索挖掘实验室（北京市海量语言信息处理与云计算应用工程技术研究中心））、thulac（清华大学自然语言处理与社会人文计算实验室）等。比较常用的是"jieba"分词器，意思就是像结巴一样结结巴巴的把一个句子分成几个单词。

jieba分词器需要单独安装，安装也很简单，直接使用pip install jieba即可。

jieba分词支持说那种分词模式：
	1. 精确模式：试图将句子最精确的切开，适合于文本分析。
	2. 全模式：把句子中所有的可以成词的词语都扫描出来，速度非常快，但不能解决歧义。
	3. 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合于搜索引擎分词，支持繁体分词。

下面分别用着三种模式来进行中文分词，比较其异同点。

```Python
# 中文分词应该使用jieba等分词模块
import jieba
text = "这是【火炉炼AI】的机器学习系列文章，本文的标题是《【火炉炼AI】机器学习034-NLP对文本进行分词》。"+ \
    "你可以从这个系列文章中学习到很多关于机器学习，人工智能方面的基础知识和实战技巧。"+\
    "请尽情享受吧！我的AI朋友们。。。"
mode1_list=jieba.cut(text,cut_all=False) # 精确模式（默认为False,精确模式）
print('jieba-精确模式结果：')
print('/'.join(mode1_list))

mode2_list=jieba.cut(text,cut_all=True) # 全模式
print('jieba-全模式结果：')
print('/'.join(mode2_list))

mode3_list=jieba.cut_for_search(text) # 搜索引擎模式
print('jieba-搜索引擎模式结果：')
print('/'.join(mode3_list))
```

**-------------------------------------输---------出--------------------------------**

这是/【/火炉/炼/AI/】/的/机器/学习/系列/文章/，/本文/的/标题/是/《/【/火炉/炼/AI/】/机器/学习/034/-/NLP/对/文本/进行/分词/》/。/你/可以/从/这个/系列/文章/中/学习/到/很多/关于/机器/学习/，/人工智能/方面/的/基础知识/和/实战/技巧/。/请/尽情/享受/吧/！/我/的/AI/朋友/们/。/。/。
jieba-全模式结果：
这/是///火炉/炼/AI//的/机器/学习/系列/文章///本文/的/标题/是////火炉/炼/AI//机器/学习/034/NLP/对/文本/进行/分词////你/可以/从/这个/系列/文章/中学/学习/到/很多/关于/机器/学习///人工/人工智能/智能/方面/的/基础/基础知识/知识/和/实战/战技/技巧///请/尽情/享受/吧///我/的/AI/朋友/们////
jieba-搜索引擎模式结果：
这是/【/火炉/炼/AI/】/的/机器/学习/系列/文章/，/本文/的/标题/是/《/【/火炉/炼/AI/】/机器/学习/034/-/NLP/对/文本/进行/分词/》/。/你/可以/从/这个/系列/文章/中/学习/到/很多/关于/机器/学习/，/人工/智能/人工智能/方面/的/基础/知识/基础知识/和/实战/技巧/。/请/尽情/享受/吧/！/我/的/AI/朋友/们/。/。/。

**--------------------------------------------完-------------------------------------**

从上面的输出结果可以看出，这三种方法还是有很多不同点的，比如全模式的结果都没有标点符号，而且有几个地方有重复，比如"文章中学习到"被分割成/文章/中学/学习/到/，而搜索引擎模式则对长词在此切分，所以有些地方重复，比如/人工/智能/人工智能/方面/的/基础/知识/基础知识/等。

**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#小\*\*\*\*\*\*\*\*\*\*结\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**

**1，如果想对英文进行分词，可以直接使用NLP中自带的sent_tokenize，word_tokenize，WordPunctTokenizer分词器，其中的word_tokenize是应用最频繁的。**

**2，如果想对中文进行分词，可以使用jieba模块，其中的精确模式（也是其默认模式）已经能够很好的将文本切分开来。其他模式在不同的应用场景中会用到。**

**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**


<br/>

注：本部分代码已经全部上传到（[**我的github**](https://github.com/RayDean/MachineLearning)）上，欢迎下载。

参考资料:

1, Python机器学习经典实例，Prateek Joshi著，陶俊杰，陈小莉译