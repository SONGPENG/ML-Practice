【火炉炼AI】机器学习030-KNN分类器模型的构建
-

(本文所使用的Python库和版本号: Python 3.6, Numpy 1.14, scikit-learn 0.19, matplotlib 2.2 )

KNN(K-nearest neighbors)是用K个最近邻的训练数据集来寻找未知对象分类的一种算法。其基本的核心思想在我的上一篇文章中介绍过了。

<br/>

## 1. 准备数据集

此处我的数据集准备包括数据加载和数据可视化，这部分比较简单，以前文章中使用了多次，直接看数据分布图。

![样本数据分布图](https://i.imgur.com/VN5Wo1v.png)


<br/>

## 2. 构建KNN分类器模型

### 2.1 KNN分类器模型的构建和训练

构建KNN分类器模型的方法和SVM，RandomForest的方法类似，代码如下：

```Python

# 构建KNN分类模型
from sklearn.neighbors import KNeighborsClassifier
K=10 # 暂定10个最近样本
KNN=KNeighborsClassifier(K,weights='distance')
KNN.fit(dataset_X,dataset_y) # 使用该数据集训练模型

```

上面使用数据集训练了这个KNN模型，但是我们怎么知道该模型的训练效果了？下面绘制了分类模型在训练数据集上的分类效果，从边界上来看，该分类器比较清晰的将这个数据集区分开来。

![KNN分类模型在训练集上的分类效果](https://i.imgur.com/wYbcTNO.png)


### 2.1 用训练好的KNN分类器预测新样本

直接上代码：

```Python

# 用训练好的KNN模型预测新样本
new_sample=np.array([[4.5,3.6]])
predicted=KNN.predict(new_sample)[0]
print("KNN predicted：{}".format(predicted))

```

得到的结果是2，表示该新样本属于第2类。

下面我们将这个新样本绘制到图中，看看它在图中的位置。

为了绘制新样本和其周围的K个样本的位置，我修改了上面的plot_classifier函数，如下为代码：

```Python
# 为了查看新样本在原数据集中的位置，也为了查看新样本周围最近的K个样本位置，
# 我修改了上面的plot_classifier函数，如下所示：

def plot_classifier2(KNN_classifier, X, y,new_sample,K):
    x_min, x_max = min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0 # 计算图中坐标的范围
    y_min, y_max = min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0
    step_size = 0.01 # 设置step size
    x_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size),
                                     np.arange(y_min, y_max, step_size))
    # 构建网格数据
    mesh_output = KNN_classifier.predict(np.c_[x_values.ravel(), y_values.ravel()])
    mesh_output = mesh_output.reshape(x_values.shape) 
    plt.figure()
    plt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black', 
                linewidth=1, cmap=plt.cm.Paired)
    # 绘制新样本所在的位置
    plt.scatter(new_sample[:,0],new_sample[:,1],marker='*',color='red')
    # 绘制新样本周围最近的K个样本，只适用于KNN
    # Extract k nearest neighbors
    dist, indices = KNN_classifier.kneighbors(new_sample)
    plt.scatter(dataset_X[indices][0][:][:,0],dataset_X[indices][0][:][:,1],
                marker='x',s=80,color='r')
    # specify the boundaries of the figure
    plt.xlim(x_values.min(), x_values.max())
    plt.ylim(y_values.min(), y_values.max())

    # specify the ticks on the X and Y axes
    plt.xticks((np.arange(int(min(X[:, 0])), int(max(X[:, 0])), 1.0)))
    plt.yticks((np.arange(int(min(X[:, 1])), int(max(X[:, 1])), 1.0)))

    plt.show()

```

直接代入运行后得到结果图：

![新样本在KNN分类器数据点中的位置](https://i.imgur.com/LgZLAnk.png)

从图中可以看出，红色的五角星是我们的新样本，而红色的叉号表示与其最近的K个邻居。可以看出，这些邻居中的大多数都位于第二个类别中，故而新样本也被划分到第二个类比，通过predict得到的结果也是2。


**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#小\*\*\*\*\*\*\*\*\*\*结\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**

**1，构建和训练KNN分类器非常简单，只需要用sklearn导入KNNClassifier，然后用fit（）函数即可。**

**2，KNN分类器存储了所有可用的训练集数据点，在新的数据点需要预测时，首先计算该新数据点和内部存储的所有数据点的相似度（也就是距离），并对该距离排序，获取距离最近的K个数据点，然后判断这K个数据点的大多数属于哪一个类别，就认为该新数据点属于哪一个类别。这也解释了为什么K通常取奇数，要是偶数，得到两个类别的数据点个数都相等，那就尴尬了。**

**3，KNN分类器的难点是寻找最合适的K值，这个需要用交叉验证来反复尝试，采用具有最大准确率或召回率的K作为最佳Ｋ值，这个过程也可以采用GridSearch或RandomSearch来完成。**

**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**


<br/>

注：本部分代码已经全部上传到（[**我的github**](https://github.com/RayDean/MachineLearning)）上，欢迎下载。

参考资料:

1, Python机器学习经典实例，Prateek Joshi著，陶俊杰，陈小莉译