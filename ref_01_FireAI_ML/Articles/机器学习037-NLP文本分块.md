【火炉炼AI】机器学习037-NLP文本分块
-

(本文所使用的Python库和版本号: Python 3.6, Numpy 1.14, scikit-learn 0.19, matplotlib 2.2， NLTK 3.3)

文本分块是将一大段文本分割成几段小文本，其目的是比如想获取一段文本中的一小部分，或分割得到固定单词数目的小部分等，经常用于非常大的文本。注意文本分块和分词不一样，分词的目的是把一段文本分割成单词，而文本分块的目的是把一大段文本分割成多个小段文本。

<br/>

## 1. NLP文本分块

在不用的应用中，可能需要按照不同的规则对大段文本进行分块，此处我们需要得到单词数相等的块，故而可以编写函数来实现这种规则的分块。代码如下。

```Python
from nltk.tokenize import word_tokenize
def split(dataset,words_num):
    '''
    将dataset这一整段文本分割成N个小块，
    使得每个小块中含有单词的数目等于words_num'''
    words=dataset.split(' ') # 此处用空格来区分单词是否合适？
    # words=word_tokenize(dataset) # 用分词器来分词是否更合适一些？
    
    rows=int(np.ceil(len(words)/words_num)) # 即行数
    result=[] # 预计里面装的元素是rows行words_num列，最后一行可能少于words_num，故不能用np.array

    # words是list，可以用切片的方式获取
    for row in range(rows):
        result.append(words[row*words_num:(row+1)*words_num])
    return result

```

然后用简·奥斯丁的《爱玛》中的文本作为数据集，由于这个数据集太大，长度有192427，故而我们此处只获取前面的1000个单词做测试。

```Python
# 测试一下
# 数据集暂时用简·奥斯丁的《爱玛》中的文本
dataset=nltk.corpus.gutenberg.words('austen-emma.txt')
print(len(dataset)) # 192427 代表读入正常
result=split(" ".join(dataset[:1000]), 30) # 只取前面的1000个单词，每30个单词分一个块，一共有34个块
print(len(result))
print(result[0])
print(len(result[0]))
print(result[-1])
print(len(result[-1]))
```
**-------------------------------------输---------出--------------------------------**

192427
34
['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed']
30
['its', 'separate', 'lawn', ',', 'and', 'shrubberies', ',', 'and', 'name', ',']
10

**--------------------------------------------完-------------------------------------**

可以看出split之后的分成了34块，第一个块长度是30，而最后一块的长度是10，并且split函数准确的将文本进行了分块。

**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#小\*\*\*\*\*\*\*\*\*\*结\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**

**1，本例中文本分块貌似没有用到NLTK模块中的任何函数，只用python字符串处理函数就可以。但在其他应用场景中，可能会需要更复杂的函数来完成特定的分块功能。**

**2，本例中使用空格来区分一个单词，这种分词方式并不一定准确，可以使用前面讲到的word_tokenize函数来分词，可能更准确一些。**

**3，如果是中文的分块，可以先用jieba对文本进行分词，然后在获取特定的单词数来进行文本分块，仿照上面的split函数很容易扩展到中文方面，此处省略。**

**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**


<br/>

注：本部分代码已经全部上传到（[**我的github**](https://github.com/RayDean/MachineLearning)）上，欢迎下载。

参考资料:

1, Python机器学习经典实例，Prateek Joshi著，陶俊杰，陈小莉译
