{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze, a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "# 对句子进行分割（tokenization)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze, a couple of sentences with punctuations to see it in action.\"\n",
    "sent_list=sent_tokenize(text)\n",
    "print(sent_list) # 句子结尾符号为标志分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', ',', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "# 对句子进行分词，即根据语义将一句话分成多个单词\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze, a couple of sentences with punctuations to see it in action.\"\n",
    "word_list=word_tokenize(text)\n",
    "print(word_list) # 以空格，问号，感叹号，句号等分割成单词\n",
    "# 分词在NLP中非常重要，经常是NLP的第一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还有一种分词方法： PunktWordTokenizer也是以标点符号分割文本，但是不分割单词中的标点符号\n",
    "# from nltk.tokenize import PunktWordTokenizer\n",
    "# PunktWordTokenizer已经不用了，故而没法import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', ',', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "# 还有一个分词方法：WordPunct分词器，可以将标点符号保留到不同的句子标记中\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze, a couple of sentences with punctuations to see it in action.\"\n",
    "word_punct=WordPunctTokenizer()\n",
    "word_punct_list=word_punct.tokenize(text)\n",
    "print(word_punct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是【火炉炼AI】的机器学习系列文章，本文的标题是《【火炉炼AI】机器学习034-NLP对文本进行分词》。你可以从这个系列文章中学习到很多关于机器学习，人工智能方面的基础知识和实战技巧。请尽情享受吧！我的AI朋友们。。。']\n",
      "['这是【火炉炼AI】的机器学习系列文章，本文的标题是《【火炉炼AI】机器学习034-NLP对文本进行分词》。你可以从这个系列文章中学习到很多关于机器学习，人工智能方面的基础知识和实战技巧。请尽情享受吧！我的AI朋友们。。。']\n"
     ]
    }
   ],
   "source": [
    "# 对中文进行分词\n",
    "text = \"这是【火炉炼AI】的机器学习系列文章，本文的标题是《【火炉炼AI】机器学习034-NLP对文本进行分词》。\"+ \\\n",
    "    \"你可以从这个系列文章中学习到很多关于机器学习，人工智能方面的基础知识和实战技巧。\"+\\\n",
    "    \"请尽情享受吧！我的AI朋友们。。。\"\n",
    "sent_list=sent_tokenize(text)\n",
    "print(sent_list) # 句子结尾符号为标志分割\n",
    "word_list=word_tokenize(text)\n",
    "print(word_list) # 以空格，问号，感叹号，句号等分割成单词\n",
    "# 表示对中文完全无效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba-精确模式结果：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.983 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是/【/火炉/炼/AI/】/的/机器/学习/系列/文章/，/本文/的/标题/是/《/【/火炉/炼/AI/】/机器/学习/034/-/NLP/对/文本/进行/分词/》/。/你/可以/从/这个/系列/文章/中/学习/到/很多/关于/机器/学习/，/人工智能/方面/的/基础知识/和/实战/技巧/。/请/尽情/享受/吧/！/我/的/AI/朋友/们/。/。/。\n",
      "jieba-全模式结果：\n",
      "这/是///火炉/炼/AI//的/机器/学习/系列/文章///本文/的/标题/是////火炉/炼/AI//机器/学习/034/NLP/对/文本/进行/分词////你/可以/从/这个/系列/文章/中学/学习/到/很多/关于/机器/学习///人工/人工智能/智能/方面/的/基础/基础知识/知识/和/实战/战技/技巧///请/尽情/享受/吧///我/的/AI/朋友/们////\n",
      "jieba-搜索引擎模式结果：\n",
      "这是/【/火炉/炼/AI/】/的/机器/学习/系列/文章/，/本文/的/标题/是/《/【/火炉/炼/AI/】/机器/学习/034/-/NLP/对/文本/进行/分词/》/。/你/可以/从/这个/系列/文章/中/学习/到/很多/关于/机器/学习/，/人工/智能/人工智能/方面/的/基础/知识/基础知识/和/实战/技巧/。/请/尽情/享受/吧/！/我/的/AI/朋友/们/。/。/。\n"
     ]
    }
   ],
   "source": [
    "# 中文分词应该使用jieba等分词模块\n",
    "import jieba\n",
    "text = \"这是【火炉炼AI】的机器学习系列文章，本文的标题是《【火炉炼AI】机器学习034-NLP对文本进行分词》。\"+ \\\n",
    "    \"你可以从这个系列文章中学习到很多关于机器学习，人工智能方面的基础知识和实战技巧。\"+\\\n",
    "    \"请尽情享受吧！我的AI朋友们。。。\"\n",
    "mode1_list=jieba.cut(text,cut_all=False) # 精确模式（默认为False,精确模式）\n",
    "print('jieba-精确模式结果：')\n",
    "print('/'.join(mode1_list))\n",
    "\n",
    "mode2_list=jieba.cut(text,cut_all=True) # 全模式\n",
    "print('jieba-全模式结果：')\n",
    "print('/'.join(mode2_list))\n",
    "\n",
    "mode3_list=jieba.cut_for_search(text) # 搜索引擎模式\n",
    "print('jieba-搜索引擎模式结果：')\n",
    "print('/'.join(mode3_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
